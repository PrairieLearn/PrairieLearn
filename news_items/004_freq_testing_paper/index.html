<div class="d-flex justify-content-center">
  <div class="card mb-4" style="width: 80%;">
    <div class="card-body bg-light">
      Morphew, J. W., Silva, M., Herman, G., and West, M., <b>"Frequent mastery testing with second-chance exams leads to enhanced student learning in undergraduate engineering"</b>, <i>Applied Cognitive Psychology</i> 34(1), 2020. DOI: <a href="https://doi.org/10.1002/acp.3605">10.1002/acp.3605</a>. Full-text link: <a href="http://lagrange.mechse.illinois.edu/pubs/MoSiHeWe2020/MoSiHeWe2020.pdf">PDF</a>.
    </div>
  </div>
</div>

<p>
  In this recently published paper, <a href="https://engineering.purdue.edu/ENE/People/ptProfile?resource_id=222158">Jason Morphew</a> and co-authors showed that frequent short PrairieLearn exams in the CBTF (Computer-Based Testing Facility) substantially increased student learning in TAM 251 ("Introductory Solid Mechanics") at the University of Illinois. Most impressively, these learning gains were on an identical pen-and-paper final exam and there were improvements at both high and low grade levels.
</p>

<p>
  <b>How did TAM 251 use the CBTF?</b> In previous semesters the course used a <i>traditional exam schedule</i> with two pen-and-paper midterm exams. <a href="https://mfsilva.web.illinois.edu">Mariana Silva</a> converted it to a <i>frequent exam schedule</i> with seven short CBTF exams and seven optional second-chance retry exams (see Figure 1). The new exams were all implemented in PrairieLearn and allowed students to re-attempt questions for partial credit, with a mastery grading scheme that only awarded points for correct answers.
</p>

<div class="d-flex justify-content-center">
  <div class="card border-1 mt-2 mb-4" style="width: 90%;">
    <div class="text-center m-4">
      <a href="exam_schedule.png"><img src="exam_schedule.png" class="card-img-top" alt="Traditional versus frequent exam schedules."></a>
    </div>
    <div class="card-body d-flex justify-content-center bg-light">
      <p class="card-text">
        <b>Figure 1:</b> Traditional versus frequent exam schedule in TAM 251.
      </p>
    </div>
  </div>
</div>

<p>
  <b>How was learning measured?</b> Two semesters were compared, one using the traditional exam schedule and the other using frequent exams. Both semesters finished with an identical pen-and-paper final exam that consisted of 13 multiple-choice questions and three free-response questions in the given–find–solution format. In each semester all students took the exam at the same time in a proctored facility and the exam papers were kept secret and never released to students. The exam was hand-graded using an identical detailed rubric in each semester. The statistical analysis controlled for the ability of the incoming student population using grades in the prerequisite course.
</p>

<p>
  <b>How much did student learning improve?</b> Students using the frequent exam schedule scored 7% higher on average (about 2/3 of a letter grade) on the final exam. This resulted in twice the number of A grades on the final and a drop of 60% in D and F grades (see Figure 2). The improvement was actually higher on the free-response questions (+11.3 percentage points) than the multiple-choice questions (+3.5 points), despite the fact that students had not practiced pen-and-paper free-response questions before in this class. There were over 200 students in each of the traditional- and frequent-exam semesters so all the results are statistically significant. As detailed in the paper, women and minority students had the same increases as the rest of the population.
</p>

<div class="d-flex justify-content-center">
  <div class="card border-1 mt-2 mb-4" style="width: 80%;">
    <div class="text-center m-4">
      <a href="exam_score_dist.png"><img src="exam_score_dist.png" class="card-img-top" style="max-width: 40rem;" alt="Histogram of exam scores."></a>
    </div>
    <div class="card-body d-flex justify-content-center bg-light">
      <p class="card-text">
        <b>Figure 2:</b> Histogram of student results on the identical pen-and-paper final exam for the two semesters. Exam letter grades correspond to the following exam score ranges: A (90% or higher), B (80% to 90%), C (70% to 80%), and D/F (below 70%). Error bars show 95% confidence intervals.
      </p>
    </div>
  </div>
</div>

<p>
  <b>What's the theory behind these results?</b> Overall, it seems likely that students both study more and use more effective study strategies when given frequent tests, for at least three reasons. First, having many small exams essentially forces students to use <a href="https://en.wikipedia.org/wiki/Distributed_practice">distributed practice</a> rather than cramming. Second, testing is simply more effective at solidifying memory than just studying (the <a href="https://en.wikipedia.org/wiki/Testing_effect">testing effect</a> in psychology). Third, second-chance exams and mastery grading encourage students to learn from their errors on the first-chance exams, rather than simply dismissing the results as a fluke or the result of an unfair test, thereby improving <a href="https://en.wikipedia.org/wiki/Metacognition">metacognition</a>.
</p>

<hr class="mt-5">
<p class="text-right small">
  This post is also available at <a href="https://edatscale.org/2020/02/26/learning-gains-from-frequent-computerized-exams/">edatscale.org</a>
</p>
